\chapter{Conclusions}
\label{conclusions}
The main question that we attempted to answer with our research work is, basically, \emph{to what extent classic \ac{ID} approaches can be adapted and integrated to mitigate todays' Internet threats}. Examples of such threats include attacks against web applications like \ac{SQL} injections, client\hyp{}side malware\index{malware} that force the browser to download viruses or connect to botnets, and so forth. Typically, these attempts are labeled as \emph{malicious activities}. The short answer to the question is the following.

As long as the technologies (e.g., applications, protocols, devices) will prevent us to reach a sophistication level such that malicious activity is seamlessly camouflaged as normal activity, then \ac{ID} techniques will constitute an effective countermeasure. This is because \acp{IDS} --- in particular those that leverage anomaly\hyp{}based techniques --- are specifically designed to detect unexpected events in a computer infrastructure. The crucial point of anomaly\hyp{}based techniques is they are conceived to be generic. In principle, they indeed make no difference between an alteration of a process' control flow caused by an attempt to interpret a crafted \textsf{JavaScript} code and one due to a buffer overflow being exploited. Thus, as long as the benign activity of a system is relatively simple to model, then \ac{ID} techniques are the building block of choice to design effective protections.

Like many other researches, this work is meant to be a longer and more articulated answer to the aforementioned question. The concluding remarks of Chapter~\ref{host}, \ref{web}, and \ref{correlation}, summarize the results of our contributions to mitigate the issues we identified in host and web \acp{IDS}, and alert correlation systems. A common line to the works presented in this thesis is the problem of false detections, which is certainly one of the most significant barriers to the wide adoption of anomaly\hyp{}based systems. In fact, the tools that are available to the public already offer superb detection capabilities that can recognize all the known \footnote{Recall that a large portion of the systems proposed in the literature have been tested using \textsf{Snort} as a baseline, as we argue in Section~\ref{detection:evaluation:issues}} threats and, in theory, are effective also against unknown malicious activities. However, a large share of the alerts fired by these tools are negligible; either because they regard threats that do not apply to the actual scenario (e.g., unsuccessful attacks or vulnerable software version mismatches) or because the recognized anomaly does not reflect an attack at all (e.g., a software is upgraded and a change is confused with an threat). False detections mean time spent by the security officers to investigate the possible causes of the attack, thus, false detections mean costs.

We demonstrated that most of the false detections, especially false positives, can be prevented by carefully designing the models of normal activity. For example, we have been able to suppress many false positives caused by too strict model constraints. In particular, we substituted the ``crisp'' checks performed by deterministic relations learned over system calls' arguments with ``smoother'' models (e.g., Gaussian distributions instead of simple ranges) that, as we shown in Section~\ref{host:improving:improved-models}, preserve the detection capabilities and decrease the rate of false alerts.

We also have shown that another good portion of false positives can be avoided by solving training issues. In particular, our contributions on detection of attacks against web applications have identified that, in certain cases, training is responsible for more than 70\% of the false positives. We proposed to solve this issue by dynamically updating models of benign activity while the system is running. Even though our solution may pose new, limited risks in some situations, it is capable of suppressing all the false detections due to incomplete training; and, given the low base-rate of attacks we pointed out in Section~\ref{detection:evaluation:base-rate-fallacy}, the resulting system offers a good balance between protection and costs (due to false positives).

Another important, desirable feature of an \ac{IDS} is its capability of recognizing logically\hyp{}related alerts. Note that, however, this is nothing but a slightly more sophisticated alert reduction mechanism, which once again means decreasing the effort of the security officer. In fact, as we have shown in the last chapter of this work, alert aggregation techniques can be leveraged to reduce the amount of false detections, not just for compressing them into more compact reports. However, alert correlation is a very difficult task and many efforts have been proposed to address it. Our point is that the research on this topic is still very limited and no common directions can be identified as we highlighted in Section~\ref{detection:conclusions}.

\medskip

The main future directions of our research have been already delineated in each chapter's conclusions. In addition, there are a few ideas that are not mature enough and thus have not been developed yet, even though they were planned to be included in this work. Regarding client-side protection mechanisms for browsers, we are designing a \textsf{Firefox} extension to learn the structure of benign web pages in terms of objects of different types typically contained. Here, the term ``structure'' refers to both the \emph{types} of objects (i.e., images, videos) and the \emph{order} these objects are fetched with and from which domains. We believe that this is a minimal set of features that can be used to design very simple classification features to distinguish between legit and malicious pages. One of our goals is to detect pages that contain an unexpected ``redirection pattern'' due to \texttt{<iframe />}s leveraged by drive-by downloads.

Regarding server-side protection techniques we are currently investigating the possibility of mapping some features of an \ac{HTTP} request, such as the order of the parameters or the characteristics of their content, with the resulting \ac{SQL} query, if any. If such a mapping is found, many of the existing anomaly detection techniques used to protect the \acp{DB} can be coupled with web\hyp{}based \ac{IDS} to offer a double layer protection mechanism. This idea can be leveraged to distinguishing between high-risk requests, which permanently modify a resource, and low-risk ones, which only temporarily affect a web page.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
